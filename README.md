# ğŸ¤– Chatbot IA - WhatsApp & Telegram

![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)
![Flask](https://img.shields.io/badge/Flask-3.1.0-lightgrey.svg)
![Docker](https://img.shields.io/badge/Docker-Ready-2496ED.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![AI](https://img.shields.io/badge/AI-OpenAI%20%7C%20Gemini%20%7C%20Ollama-orange.svg)

## ğŸš€ Sistema Inteligente de Chatbot Multi-Proveedor con IA

Chatbot avanzado con soporte para **mÃºltiples proveedores de IA** (OpenAI, Gemini, Ollama) para conversaciones inteligentes a travÃ©s de WhatsApp y Telegram, con arquitectura limpia, principios SOLID, RAG (Retrieval Augmented Generation) y sistema automÃ¡tico de limpieza de contexto.

---

## âœ¨ CaracterÃ­sticas Principales

### ğŸ¤– **Inteligencia Artificial Multi-Proveedor**
- **OpenAI GPT-4o** / **GPT-4o-mini** - Conversaciones naturales con el modelo lÃ­der
- **Google Gemini 2.5** - IA multimodal de prÃ³xima generaciÃ³n
- **Ollama** - Modelos locales y en la nube (soporte para modelos customizados)
- **Cambio dinÃ¡mico de proveedor** - Configurable via variable de entorno
- **AnÃ¡lisis de imÃ¡genes** con visiÃ³n por computadora
- **Procesamiento de documentos** (PDF, Word, texto)
- **BÃºsqueda web** integrada con SerpAPI

### ğŸ¯ **RAG (Retrieval Augmented Generation)**
- **Embeddings multi-proveedor**:
  - OpenAI: `text-embedding-3-small` (1536 dims)
  - Gemini: `gemini-embedding-001` (768 dims)
  - Ollama: `embeddinggemma`, `qwen3-embedding` (768 dims)
- **ChromaDB** como vector store
- **BÃºsqueda semÃ¡ntica** para contexto relevante
- **IndexaciÃ³n automÃ¡tica** de documentos

### ğŸ’¬ **Multi-Canal**
- **WhatsApp** - IntegraciÃ³n completa con Meta API
- **Telegram** - Bot nativo con todas las funciones

### ğŸ—ï¸ **Arquitectura Avanzada**
- **Clean Architecture** con separaciÃ³n por capas (Domain, Application, Infrastructure)
- **Principios SOLID** aplicados completamente
- **Dependency Injection** para bajo acoplamiento
- **Strategy Pattern** para algoritmos intercambiables
- **Factory Pattern** para creaciÃ³n de proveedores
- **Adapter Pattern** para unificaciÃ³n de interfaces
- **Memoria vectorial** para bÃºsqueda semÃ¡ntica
- **OptimizaciÃ³n de tokens** para mejor rendimiento
---

## ğŸ”„ Cambiar entre Proveedores de IA

El sistema soporta **3 proveedores de IA** con cambio dinÃ¡mico mediante configuraciÃ³n:

### ğŸ“Š **Comparativa de Proveedores**

| CaracterÃ­stica | OpenAI | Gemini | Ollama |
|---------------|--------|---------|---------|
| **Texto** | gpt-4o-mini<br>gpt-4o | gemini-2.5-flash-lite<br>gemini-2.0-flash | Modelos locales<br>(llama2, mistral, etc.) |
| **Embeddings** | text-embedding-3-small<br>(1536 dims) | gemini-embedding-001<br>(768 dims) | embeddinggemma<br>qwen3-embedding<br>(768 dims) |
| **Latencia** | 1-3s | 1-2s | Variable<br>(depende del hardware) |
| **Costo** | $$$ | $$ | Gratis (local)<br>$ (cloud) |
| **Offline** | âŒ No | âŒ No | âœ… SÃ­ (local) |
| **VisiÃ³n** | âœ… SÃ­ | âœ… SÃ­ | âš ï¸ Algunos modelos |
| **RAG** | âœ… SÃ­ | âœ… SÃ­ | âœ… SÃ­ |

### âš™ï¸ **ConfiguraciÃ³n por Proveedor**

#### **OpenAI**
```bash
# .env
AI_PROVIDER=openai
OPENAI_API_KEY=sk-proj-...
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
```

#### **Google Gemini**
```bash
# .env
AI_PROVIDER=gemini
GEMINI_API_KEY=AIzaSy...
GEMINI_MODEL=gemini-2.5-flash-lite
GEMINI_EMBEDDING_MODEL=gemini-embedding-001
```

#### **Ollama (Local)**
```bash
# .env
AI_PROVIDER=ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama2
OLLAMA_EMBEDDING_MODEL=embeddinggemma

# AsegÃºrate de tener Ollama corriendo:
# ollama serve
# ollama pull llama2
# ollama pull embeddinggemma
```

#### **Ollama (Cloud)**
```bash
# .env
AI_PROVIDER=ollama
OLLAMA_URL=https://api.ollama.com
OLLAMA_MODEL=tu-modelo-cloud
OLLAMA_EMBEDDING_MODEL=embeddinggemma
OLLAMA_API_KEY=tu-api-key
```

### ğŸ”„ **Cambio en Tiempo Real**
```bash
# Cambiar proveedor (requiere reiniciar contenedor)
docker compose down
# Editar .env con nuevo AI_PROVIDER
docker compose up -d
```

---

## ğŸ¯ RAG (Retrieval Augmented Generation)

### ğŸ“š **CÃ³mo Funciona**

1. **IndexaciÃ³n**: Los documentos se dividen en chunks y se generan embeddings
2. **BÃºsqueda**: Las consultas se convierten en embeddings y se buscan chunks similares
3. **GeneraciÃ³n**: El LLM usa los chunks relevantes como contexto para responder

### ğŸ”§ **ConfiguraciÃ³n RAG**

```bash
# .env
RAG_ENABLED=True
RAG_CHUNK_SIZE=500
RAG_CHUNK_OVERLAP=50
RAG_TOP_K=5
RAG_MIN_SIMILARITY=0.7
```

### ğŸ“ **Ingestar Documentos**

```bash
# Texto directo
curl -X POST http://localhost:9001/service_ia/api/rag/ingest \
  -F "user_id=user123" \
  -F "text=Contenido del documento..." \
  -F "title=Mi Documento"

# Archivo PDF/DOCX
curl -X POST http://localhost:9001/service_ia/api/rag/ingest/file \
  -F "user_id=user123" \
  -F "file=@documento.pdf" \
  -F "title=Manual de Usuario"
```

### ğŸ” **Buscar Contexto**

```bash
# BÃºsqueda semÃ¡ntica
curl "http://localhost:9001/service_ia/api/rag/search?query=Â¿CÃ³mo+configurar+el+sistema?&top_k=5"

# BÃºsqueda por usuario
curl "http://localhost:9001/service_ia/api/rag/search?user_id=user123&query=mi+consulta"
```

### ğŸ—‘ï¸ **Eliminar Documentos**

```bash
curl -X DELETE http://localhost:9001/service_ia/api/rag/documents/doc-id-123
```

### ğŸ¯ **Embeddings por Proveedor**

El sistema **automÃ¡ticamente usa el modelo de embeddings** correspondiente al proveedor seleccionado:

- **OpenAI**: Usa `text-embedding-3-small` para generar vectores de 1536 dimensiones
- **Gemini**: Usa `gemini-embedding-001` optimizado para RAG (768 dims)
- **Ollama**: Usa modelos especÃ­ficos como `embeddinggemma` o `qwen3-embedding` (768 dims)

**Ejemplo de configuraciÃ³n mixta** (no recomendado):
```bash
# Si quieres usar OpenAI para chat pero Ollama para embeddings,
# necesitarÃ­as dos instancias o modificar el cÃ³digo.
# Por defecto, ambos usan el mismo proveedor configurado en AI_PROVIDER.
```

---

## ğŸ› ï¸ InstalaciÃ³n RÃ¡pida

git clone <repository-url>
cd Chatbot_IA
source env/bin/activate
pip install -r requirements.txt
```

### 2. **Configurar variables de entorno**
```bash
# Copia y edita las variables necesarias
cp .env.example .env
nano .env
```

**Variables principales:**
```bash
# SelecciÃ³n de proveedor de IA (openai, gemini, ollama)
AI_PROVIDER=gemini

# OpenAI (si usas AI_PROVIDER=openai)
OPENAI_API_KEY=tu-clave-openai
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Google Gemini (si usas AI_PROVIDER=gemini)
GEMINI_API_KEY=tu-clave-gemini
GEMINI_MODEL=gemini-2.5-flash-lite
GEMINI_EMBEDDING_MODEL=gemini-embedding-001

# Ollama (si usas AI_PROVIDER=ollama)
OLLAMA_URL=http://localhost:11434  # o https://api.ollama.com para cloud
OLLAMA_MODEL=llama2  # Modelo para generaciÃ³n de texto
OLLAMA_EMBEDDING_MODEL=embeddinggemma  # Modelo para embeddings
OLLAMA_API_KEY=tu-api-key-opcional  # Solo si usas Ollama Cloud

# Canales de mensajerÃ­a
TOKEN_WHATSAPP=tu-token-whatsapp
PHONE_NUMBER_ID=tu-numero-whatsapp
TELEGRAM_TOKEN=tu-token-telegram

# BÃºsqueda web (opcional)
SERPAPI_KEY=tu-clave-serpapi

# RAG (Retrieval Augmented Generation)
RAG_ENABLED=True
CHROMA_HOST=chroma
CHROMA_PORT=8000
```
### 3. **Ejecutar la aplicaciÃ³n**
```bash
cd openIAService
python main.py
```

### 4. (Opcional) Ejecutar con Docker Compose
```bash
cp .env.example .env
docker compose up --build
```
Servicios:
- App Flask: http://localhost:9001
- ChromaDB: http://localhost:9000 (vector store para RAG)

Nota sobre dependencias en Docker
- La imagen Docker usa un set mÃ­nimo en `requirements-base.txt` para garantizar builds estables (Flask, OpenAI, OCR/documentos, Telegram, ChromaDB, RAG).
- El archivo `requirements.txt` contiene librerÃ­as opcionales adicionales (MCP, FastAPI stack, proveedores extra) que pueden tener conflictos entre sÃ­. Si necesitas incluirlas en la imagen, avÃ­same y preparo perfiles/targets de build especÃ­ficos.

---

## ğŸ“Š Monitoreo y Logs

### ğŸ” **Monitor de Logs**
```bash
# Monitorear log principal
./monitor_logs.sh app
./monitor_logs.sh telegram


./monitor_logs.sh all

# Estado de logs
./monitor_logs.sh status
```

### ğŸ“ **UbicaciÃ³n de Logs**
- **`openIAService/logs/app.log`** - Log principal
- **`openIAService/logs/telegram.log`** - Eventos Telegram
- **`openIAService/logs/whatsapp.log`** - Eventos WhatsApp

---

## ğŸŒ API Endpoints

### ğŸ“± **Webhooks**
```bash
POST /webhook/whatsapp    # Webhook WhatsApp (v1)
POST /webhook/telegram    # Webhook Telegram (v1)
POST /api/v2/webhook/whatsapp  # Webhook WhatsApp mejorado
POST /api/v2/webhook/telegram  # Webhook Telegram mejorado
```

### ğŸ“Š **Monitoreo**
```bash
GET /api/v2/health               # Estado del sistema
GET /api/v2/channels/status      # Estado de canales (admin)
GET /api/v2/conversation/:id/summary # Resumen por usuario (admin)
POST /api/v2/message/send        # EnvÃ­o programÃ¡tico (admin)
POST /api/v2/ai/ollama/thinking  # Prueba thinking del proveedor IA actual
POST /api/v2/ai/ollama/stream    # Prueba streaming del proveedor IA actual
GET /api/context/status          # Estado de contextos
POST /api/context/cleanup        # Limpiar contextos manualmente
GET /api/v2/architecture/info    # InformaciÃ³n de arquitectura
```

### ğŸ§¾ Contrato de Respuestas y Errores

Todas las rutas HTTP nuevas/refactorizadas usan formato homogÃ©neo.

Respuesta exitosa:
```json
{
  "success": true,
  "data": {},
  "timestamp": "2026-02-12T12:00:00"
}
```

Respuesta de error (global handler):
```json
{
  "success": false,
  "error": "mensaje descriptivo",
  "code": "VALIDATION_ERROR",
  "timestamp": "2026-02-12T12:00:00"
}
```

CÃ³digos frecuentes:
- `VALIDATION_ERROR`
- `INVALID_JSON`
- `RAG_DISABLED`
- `MESSAGE_SEND_ERROR`
- `UNSUPPORTED_FILE_EXTENSION`
- `UNHANDLED_ERROR`

### ğŸ§  Ollama Thinking + Streaming en canales

Flags de activaciÃ³n:
```bash
OLLAMA_CHANNEL_STREAMING_ENABLED=True
OLLAMA_CHANNEL_THINKING_ENABLED=False
OLLAMA_STREAM_CHUNK_SIZE=120
OLLAMA_STREAM_MAX_UPDATES=20
```

Comportamiento por canal:
- **Telegram**: simula streaming editando un Ãºnico mensaje (`Pensando...` â†’ texto parcial â†’ respuesta final).
- **WhatsApp**: envÃ­a respuesta final; opcionalmente puede enviar aviso de procesamiento antes del resultado.

Nota: mostrar el campo `thinking` al usuario final no suele ser recomendado en producciÃ³n. Se puede mantener para debug/admin.

### ğŸ“‚ **Archivos**
```bash
GET /uploaded_files             # Lista de archivos
```

### ğŸ” RAG (Retrieval Augmented Generation)
```bash
POST /api/rag/ingest            # Ingestar texto al vector store
GET  /api/rag/search            # Buscar contexto semÃ¡ntico
DELETE /api/rag/documents/:id   # Eliminar documento indexado
```
Ejemplos:
```bash
curl -X POST http://localhost:9001/api/rag/ingest \
	-H 'Content-Type: application/json' \
	-d '{"user_id":"u1","document_id":"doc-1","title":"Manual","text":"contenido a indexar"}'

curl 'http://localhost:9001/api/rag/search?user_id=u1&query=consulta'

IntegraciÃ³n con Nginx del servidor (externo)
Si tienes un Nginx frontal (por ejemplo optimus.pegasoconsulting.net) y quieres publicar el servicio bajo /service_ia/, usa algo como:

location /service_ia/ {
		rewrite ^/service_ia/(.*)$ /$1 break;
		proxy_set_header Host $host;
		proxy_set_header X-Real-IP $remote_addr;
		proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
		proxy_set_header X-Forwarded-Proto $scheme;
		proxy_http_version 1.1;
		proxy_set_header Upgrade $http_upgrade;
		proxy_set_header Connection "upgrade";
		proxy_read_timeout 300s;
		proxy_pass http://127.0.0.1:9001/;
}

Rendimiento y lÃ­mites de recursos
- El contenedor ejecuta Gunicorn con worker_class gthread (Ã³ptimo para E/S: llamadas OpenAI, I/O).
- Ajusta concurrencia vÃ­a variables de entorno en docker-compose:
	- WEB_CONCURRENCY: nÃºmero de workers (por defecto ~CPU/2 o 2)
	- GTHREADS: threads por worker (por defecto 4)
	- GUNICORN_TIMEOUT: timeout en segundos (por defecto 300)
	- GUNICORN_KEEPALIVE: keepalive en segundos (por defecto 5)
- LÃ­mites de recursos (opcionales) en docker-compose bajo deploy.resources.limits (cpus/memory).
```

---

## ğŸ”§ CaracterÃ­sticas TÃ©cnicas

### ğŸ“¦ **Stack TecnolÃ³gico**
- **Python 3.12+**
- **Flask** + **Gunicorn** - Framework web y WSGI server
- **SQLite** - Base de datos (contexto conversacional)
- **ChromaDB** - Vector DB (RAG)
- **Proveedores de IA**:
  - **OpenAI API** (gpt-4o, gpt-4o-mini)
  - **Google Gemini API** (gemini-2.5-flash-lite)
  - **Ollama** (llama2, mistral, modelos customizados)
- **Pydantic** - ValidaciÃ³n de datos y settings
- **Beautiful Soup** - Procesamiento HTML
- **PyMuPDF** - ExtracciÃ³n de texto de PDFs
- **python-docx** - Procesamiento de documentos Word

### ğŸ›ï¸ **Arquitectura (Clean Architecture + SOLID)**
```
openIAService/
â”œâ”€â”€ domain/                    # ğŸ”µ CAPA DE DOMINIO
â”‚   â”œâ”€â”€ entities/              # Entidades de negocio
â”‚   â”œâ”€â”€ repositories/          # Interfaces de repositorios
â”‚   â””â”€â”€ value_objects/         # Objetos de valor
â”‚
â”œâ”€â”€ application/               # ğŸŸ¢ CAPA DE APLICACIÃ“N
â”‚   â”œâ”€â”€ dto/                   # Data Transfer Objects
â”‚   â”œâ”€â”€ services/              # Servicios de aplicaciÃ³n
â”‚   â””â”€â”€ use_cases/             # Casos de uso
â”‚
â”œâ”€â”€ infrastructure/            # ğŸŸ¡ CAPA DE INFRAESTRUCTURA
â”‚   â”œâ”€â”€ ai/                    # Adaptadores de proveedores IA
â”‚   â”‚   â”œâ”€â”€ openai_adapter.py
â”‚   â”‚   â”œâ”€â”€ gemini_adapter.py
â”‚   â”‚   â””â”€â”€ ollama_adapter.py
â”‚   â”œâ”€â”€ embeddings/            # Servicio de embeddings multi-proveedor
â”‚   â”œâ”€â”€ vector_store/          # ChromaDB repository
â”‚   â”œâ”€â”€ persistence/           # SQLite repository
â”‚   â”œâ”€â”€ messaging/             # WhatsApp, Telegram adapters
â”‚   â””â”€â”€ web_search/            # SerpAPI integration
â”‚
â”œâ”€â”€ core/                      # âš™ï¸ CORE
â”‚   â”œâ”€â”€ config/                # Settings, dependencies
â”‚   â”‚   â”œâ”€â”€ settings.py        # Pydantic settings
â”‚   â”‚   â””â”€â”€ dependencies.py    # DI container
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ providers.py       # AIProvider interface
â”‚   â”‚   â””â”€â”€ factory.py         # get_ai_provider() factory
â”‚   â”œâ”€â”€ exceptions/            # Custom exceptions
â”‚   â””â”€â”€ logging/               # Structured logging
â”‚
â”œâ”€â”€ services/                  # ğŸ”§ SERVICIOS
â”‚   â”œâ”€â”€ openia_service.py      # OrquestaciÃ³n IA
â”‚   â”œâ”€â”€ channel_adapters.py    # WhatsApp/Telegram unificado
â”‚   â””â”€â”€ context_cleanup_service.py
â”‚
â””â”€â”€ routes/                    # ğŸŒ API ENDPOINTS
    â”œâ”€â”€ whatsapp_routes.py
    â”œâ”€â”€ telegram_routes.py
    â”œâ”€â”€ rag_routes.py          # RAG endpoints
    â””â”€â”€ chat_routes.py         # Chat con RAG
```

### ğŸ”„ **Patrones de DiseÃ±o Implementados**
- **Repository Pattern** - AbstracciÃ³n de datos (SQLite, ChromaDB)
- **Factory Pattern** - CreaciÃ³n de proveedores IA (`get_ai_provider()`)
- **Strategy Pattern** - Algoritmos intercambiables (limpieza, procesamiento)
- **Adapter Pattern** - UnificaciÃ³n de interfaces (OpenAI, Gemini, Ollama)
- **Dependency Injection** - InversiÃ³n de dependencias (DI Container)
- **Service Locator** - Registro centralizado de servicios

---

## ğŸš€ Uso del Sistema

### ğŸ’¬ **Comandos de Chat**
El chatbot responde a mensajes naturales en espaÃ±ol e inglÃ©s con cualquier proveedor:

```
Usuario: "Hola, Â¿cÃ³mo estÃ¡s?"
Bot: "Â¡Hola! Estoy aquÃ­ para ayudarte..."

Usuario: "Analiza esta imagen" + [imagen]
Bot: [AnÃ¡lisis detallado de la imagen] (OpenAI/Gemini)

Usuario: "Busca informaciÃ³n sobre Python"
Bot: [Resultados de bÃºsqueda web + respuesta]

Usuario: "Resume este documento" + [documento]
Bot: [Resumen generado usando RAG si estÃ¡ habilitado]
```

### ğŸ“„ **Procesamiento de Documentos con RAG**
1. Sube PDFs, documentos Word o archivos de texto vÃ­a `/api/rag/ingest/file`
2. El sistema:
   - Extrae el texto
   - Lo divide en chunks
   - Genera embeddings con el proveedor configurado
   - Indexa en ChromaDB
3. Las consultas posteriores buscan en el contexto indexado
4. El LLM genera respuestas basadas en el contenido real del documento

### ğŸ” **BÃºsqueda Web**
- BÃºsquedas automÃ¡ticas cuando se necesita informaciÃ³n actualizada
- IntegraciÃ³n transparente con SerpAPI
- Resultados procesados y resumidos por IA

### ğŸ¯ **CaracterÃ­sticas Avanzadas**

#### **Fast-Path para Preguntas Simples**
El sistema detecta preguntas triviales y responde sin LLM para reducir latencia:
- "Hola", "Buenas", "Quien eres"
- Respuestas instantÃ¡neas en < 100ms

#### **OptimizaciÃ³n de Contexto**
- Limpieza automÃ¡tica cada 24 horas
- Resumen de conversaciones largas
- GestiÃ³n de tokens para reducir costos

---

## ğŸ›¡ï¸ Seguridad y Rendimiento

### ğŸ”’ **Seguridad**
- ValidaciÃ³n de tokens para todos los webhooks
- SanitizaciÃ³n de inputs de usuario
- Logs de auditorÃ­a completos
- Variables de entorno para credenciales

### âš¡ **Rendimiento**
- Limpieza automÃ¡tica de contexto (24h)
- OptimizaciÃ³n de tokens para reducir costos
- Cache de respuestas frecuentes
- Logging asÃ­ncrono para no bloquear

---

## ğŸ“ˆ MÃ©tricas y Monitoreo

### ğŸ“Š **MÃ©tricas Disponibles**
- NÃºmero de conversaciones activas
- Uso de tokens OpenAI
- Tiempo de respuesta promedio
- Errores y excepciones

### ğŸ” **Comandos de DiagnÃ³stico**
```bash
# Ver estadÃ­sticas de contexto
curl http://localhost:8082/api/context/status

# Forzar limpieza de contextos
curl -X POST http://localhost:8082/api/context/cleanup

# Estado general del sistema
curl http://localhost:8082/api/v2/health
```

---

## ğŸ”§ Troubleshooting

### âŒ **Problemas Comunes**

#### **Error 401 con Ollama embeddings**
```bash
# Problema: unauthorized (status code: 401)
# Causa: Modelo de embeddings no disponible o API key incorrecta

# SoluciÃ³n 1: Verificar que el modelo estÃ© descargado (local)
ollama pull embeddinggemma
ollama list  # Verificar que aparece embeddinggemma

# SoluciÃ³n 2: Verificar API key (cloud)
# AsegÃºrate de que OLLAMA_API_KEY estÃ© configurada correctamente en .env

# SoluciÃ³n 3: Usar modelos alternativos
OLLAMA_EMBEDDING_MODEL=qwen3-embedding  # o all-minilm
```

#### **Gemini: "model is required"**
```bash
# Problem: El modelo estÃ¡ vacÃ­o
# SoluciÃ³n: Agregar explÃ­citamente en .env
GEMINI_MODEL=gemini-2.5-flash-lite
GEMINI_EMBEDDING_MODEL=gemini-embedding-001
```

#### **RAG no encuentra resultados**
```bash
# Verificar que RAG estÃ© habilitado
RAG_ENABLED=True

# Verificar que ChromaDB estÃ© corriendo
docker compose ps  # Debe mostrar 'chroma' como 'running'

# Ver logs de ChromaDB
docker compose logs chroma

# Reiniciar ChromaDB
docker compose restart chroma
```

#### **Cambio de proveedor no surte efecto**
```bash
# AsegÃºrate de reiniciar el contenedor
docker compose down
docker compose up -d

# Verificar logs de inicializaciÃ³n
docker compose logs app | grep "initialized"
# Debe mostrar: "Ollama adapter initialized" o "Gemini generate_text"
```

### ğŸ› **Debug Logs**

```bash
# Ver logs en tiempo real
docker compose logs -f app

# Buscar errores especÃ­ficos
docker compose logs app | grep ERROR

# Ver inicializaciÃ³n de proveedores
docker compose logs app | grep -A3 "Inicializando dependencias"
```

---

## ğŸ¤ ContribuciÃ³n

### ğŸ“ **Para Desarrolladores**
1. Fork del repositorio
2. Crear rama de feature
3. Seguir principios SOLID
4. Mantener cobertura de tests
5. Documentar cambios

### ğŸ› **Reportar Issues**
- Incluir logs relevantes
- Describir pasos para reproducir
- Especificar versiÃ³n de Python
- Adjuntar configuraciÃ³n (sin credenciales)

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver [LICENSE](LICENSE) para mÃ¡s detalles.

---

## ğŸ“ Soporte

- **ğŸ“§ Email**: carvajalgabriel1407@gmail.com
- **ğŸ™ GitHub**: [gabriel1407](https://github.com/gabriel1407)
- **ğŸ“ Proyecto**: [Chatbot_IA](https://github.com/gabriel1407/Chatbot_IA)

---

*Ãšltima actualizaciÃ³n: Febrero 2026*

## ğŸ“„ Changelog Reciente

### v2.1.0 (Febrero 2026)
- âœ… **Fase 3 completada**: extracciÃ³n de generaciÃ³n de respuestas a `ResponseGenerationUseCase`
- âœ… **DI centralizado** para `MessageHandler` y `UnifiedChannelService`
- âœ… **Fase 4 iniciada y aplicada**: manejo global de errores HTTP con `APIException`
- âœ… **Rutas homogeneizadas**: `admin_routes`, `rag_routes`, `chat_routes`, `context_routes`, `file_routes`
- âœ… **SemÃ¡ntica HTTP consistente** en validaciones y errores de negocio

### v2.0.0 (Febrero 2026)
- âœ… **Soporte multi-proveedor**: OpenAI, Gemini, Ollama
- âœ… **RAG mejorado** con embeddings especÃ­ficos por proveedor
- âœ… **Arquitectura SOLID** completamente refactorizada
- âœ… **Dependency Injection** con container centralizado
- âœ… **Fast-path** para preguntas simples
- âœ… **Ollama adapter** usando librerÃ­a oficial
- âœ… **Gemini embeddings** con gemini-embedding-001

### v1.0.0 (Noviembre 2025)
- âœ… Chatbot bÃ¡sico con OpenAI
- âœ… WhatsApp y Telegram integrados
- âœ… Procesamiento de documentos
- âœ… Sistema de limpieza de contexto